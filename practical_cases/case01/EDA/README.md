# ğŸ§¹ Pipeline de Limpieza de Datos

Una pipeline completa y flexible para la limpieza y procesamiento de datos CSV, diseÃ±ada especÃ­ficamente para el anÃ¡lisis de datos de MegaMercado.

## ğŸ“ Estructura de Archivos

```
EDA/
â”œâ”€â”€ data_cleaning_pipeline.py    # Clase principal de la pipeline
â”œâ”€â”€ config.py                   # Configuraciones predefinidas
â”œâ”€â”€ ejemplo_uso_pipeline.py      # Ejemplos de uso
â”œâ”€â”€ README.md                   # Esta documentaciÃ³n
â”œâ”€â”€ data_cleaning.log           # Log de ejecuciÃ³n (generado)
â””â”€â”€ datos_limpios/              # Datos procesados (generado)
    â”œâ”€â”€ clientes_clean.csv
    â”œâ”€â”€ productos_clean.csv
    â”œâ”€â”€ ventas_clean.csv
    â”œâ”€â”€ logistica_clean.csv
    â”œâ”€â”€ proveedores_clean.csv
    â””â”€â”€ cleaning_report.txt
```

## ğŸš€ CaracterÃ­sticas Principales

### âœ¨ Funcionalidades

- **ExtracciÃ³n automÃ¡tica**: Maneja archivos CSV y ZIP
- **Limpieza inteligente**: MÃºltiples estrategias para valores faltantes
- **DetecciÃ³n de outliers**: MÃ©todos IQR y Z-Score
- **EliminaciÃ³n de duplicados**: Con opciones de subset personalizado
- **EstandarizaciÃ³n**: Tipos de datos y formato de texto
- **ğŸ” DetecciÃ³n de inconsistencias**: Sistema avanzado de validaciÃ³n
- **Logging completo**: Seguimiento detallado del proceso
- **Reportes automÃ¡ticos**: AnÃ¡lisis de calidad antes y despuÃ©s
- **ConfiguraciÃ³n flexible**: FÃ¡cil personalizaciÃ³n por dataset

### ğŸš¨ DetecciÃ³n de Inconsistencias

La pipeline incluye un **sistema avanzado de detecciÃ³n de inconsistencias** que identifica:

#### ğŸ“ Tipos de Inconsistencias Detectadas

1. **Inconsistencias de Formato**
   - Emails invÃ¡lidos
   - TelÃ©fonos mal formateados
   - Fechas con formato incorrecto
   - Inconsistencias de capitalizaciÃ³n

2. **Inconsistencias de Rango**
   - Valores fuera de rangos esperados (edades negativas, etc.)
   - Valores negativos donde deberÃ­an ser positivos
   - Rangos lÃ³gicos violados

3. **Inconsistencias Temporales**
   - Fechas futuras en campos histÃ³ricos
   - Fechas muy antiguas (posibles errores)
   - Orden cronolÃ³gico incorrecto entre fechas relacionadas

4. **Inconsistencias de Integridad Referencial**
   - Referencias a registros que no existen
   - Claves forÃ¡neas huÃ©rfanas
   - Violaciones de integridad entre tablas

5. **Inconsistencias EstadÃ­sticas**
   - Valores que se repiten excesivamente
   - Patrones secuenciales sospechosos
   - Distribuciones anÃ³malas

6. **Violaciones de Reglas de Negocio**
   - Reglas personalizadas por dominio
   - Validaciones especÃ­ficas del negocio
   - Restricciones lÃ³gicas del sistema

#### ğŸ¯ Severidad de Inconsistencias

- **ğŸ”´ CRÃTICAS**: Problemas que impiden el procesamiento (ej: referencias huÃ©rfanas)
- **ğŸŸ  ALTAS**: Problemas importantes que afectan la calidad (ej: rangos invÃ¡lidos)
- **ğŸŸ¡ MEDIAS**: Problemas moderados que requieren atenciÃ³n (ej: formatos inconsistentes)
- **ğŸŸ¢ BAJAS**: Problemas menores de estandarizaciÃ³n (ej: capitalizaciÃ³n)

### ğŸ› ï¸ Estrategias de Limpieza

1. **`drop_rows`**: Elimina filas con valores nulos (mÃ¡s estricta)
2. **`drop_columns`**: Elimina columnas con muchos nulos
3. **`fill`**: Rellena valores faltantes con estrategias inteligentes
4. **`smart`**: Combina mÃºltiples estrategias segÃºn el contexto

## ğŸ“– Uso RÃ¡pido

### Ejemplo BÃ¡sico

```python
from data_cleaning_pipeline import DataCleaningPipeline
from config import MEGAMERCADO_FILES, ECOMMERCE_CONFIG

# Configurar paths
BASE_PATH = "ruta/a/tus/datos"

# Crear pipeline
pipeline = DataCleaningPipeline(BASE_PATH)

# Ejecutar limpieza completa CON detecciÃ³n de inconsistencias
clean_data = pipeline.run_complete_pipeline(
    MEGAMERCADO_FILES, 
    ECOMMERCE_CONFIG,
    detect_inconsistencies=True  # Â¡Activar detecciÃ³n!
)

# Guardar resultados
pipeline.save_clean_data()
```

### DetecciÃ³n de Inconsistencias Independiente

```python
from inconsistency_detector import InconsistencyDetector
import pandas as pd

# Crear detector
detector = InconsistencyDetector()

# Cargar datos
df = pd.read_csv('mi_dataset.csv')

# Detectar inconsistencias
inconsistencies = detector.detect_format_inconsistencies(df, 'mi_tabla')
inconsistencies.extend(detector.detect_range_inconsistencies(df, 'mi_tabla'))

# Ver resultados
for inc in inconsistencies:
    print(f"ğŸš¨ {inc.type}: {inc.description}")
    print(f"   Casos: {inc.count}, Severidad: {inc.severity}")
```

### ConfiguraciÃ³n de Reglas de Negocio

```python
from inconsistency_detector import InconsistencyDetector

# Crear detector
detector = InconsistencyDetector()

# Definir regla personalizada
def precio_valido(df):
    """El precio debe ser mayor a cero y menor a 10000"""
    return df[(df['precio'] <= 0) | (df['precio'] > 10000)]

# AÃ±adir regla
detector.add_business_rule('productos', 'precio_valido', precio_valido, 'HIGH')

# Definir integridad referencial
detector.add_reference_mapping('ventas', 'clientes', 'id_cliente', 'id_cliente')
```

### Limpieza RÃ¡pida de un DataFrame

```python
from data_cleaning_pipeline import quick_clean_dataframe
import pandas as pd

# Cargar datos
df = pd.read_csv('datos_sucios.csv')

# Limpiar rÃ¡pidamente
df_clean = quick_clean_dataframe(
    df, 
    strategy='smart',
    remove_duplicates=True
)
```

### AnÃ¡lisis de Calidad

```python
from data_cleaning_pipeline import analyze_dataset_quality
import pandas as pd

df = pd.read_csv('mi_dataset.csv')
analyze_dataset_quality(df, "Mi Dataset")
```

## âš™ï¸ ConfiguraciÃ³n

### ConfiguraciÃ³n por Dataset

```python
mi_config = {
    'clientes': {
        'missing_strategy': 'smart',
        'missing_threshold': 0.7,
        'remove_outliers': True,
        'outlier_columns': ['edad', 'ingresos'],
        'outlier_method': 'iqr',
        'text_columns': ['nombre', 'email', 'direccion'],
        'type_mapping': {
            'fecha_registro': 'datetime',
            'activo': 'bool'
        },
        'fill_values': {
            'telefono': 'No disponible'
        },
        'duplicate_subset': ['id_cliente']
    }
}
```

### Configuraciones Predefinidas

```python
from config import (
    ECOMMERCE_CONFIG,     # Para datos de e-commerce
    FINANCIAL_CONFIG,     # Para datos financieros
    HEALTHCARE_CONFIG,    # Para datos de salud
    STRICT_CLEANING,      # Limpieza estricta
    PERMISSIVE_CLEANING,  # Limpieza permisiva
    BALANCED_CLEANING     # Limpieza balanceada
)
```

## ğŸ”§ ParÃ¡metros de ConfiguraciÃ³n

### Estrategias de Valores Faltantes

| ParÃ¡metro | DescripciÃ³n | Valores |
|-----------|-------------|---------|
| `missing_strategy` | Estrategia para valores nulos | `'drop_rows'`, `'drop_columns'`, `'fill'`, `'smart'` |
| `missing_threshold` | Umbral para eliminar columnas | `0.0` - `1.0` |
| `fill_values` | Valores especÃ­ficos para rellenar | `Dict[str, Any]` |

### DetecciÃ³n de Outliers

| ParÃ¡metro | DescripciÃ³n | Valores |
|-----------|-------------|---------|
| `remove_outliers` | Activar detecciÃ³n de outliers | `True`, `False` |
| `outlier_columns` | Columnas a analizar | `List[str]` |
| `outlier_method` | MÃ©todo de detecciÃ³n | `'iqr'`, `'zscore'` |
| `outlier_factor` | Factor multiplicador | `float` (ej: 1.5, 2.0, 3.0) |

### Procesamiento de Texto

| ParÃ¡metro | DescripciÃ³n | Valores |
|-----------|-------------|---------|
| `text_columns` | Columnas de texto a limpiar | `List[str]` |

### Tipos de Datos

| ParÃ¡metro | DescripciÃ³n | Valores |
|-----------|-------------|---------|
| `type_mapping` | Mapeo de tipos por columna | `Dict[str, str]` |

### Duplicados

| ParÃ¡metro | DescripciÃ³n | Valores |
|-----------|-------------|---------|
| `duplicate_subset` | Columnas para detectar duplicados | `List[str]` |

## ğŸ“Š MÃ©tricas de Calidad

La pipeline proporciona mÃ©tricas automÃ¡ticas de calidad:

- **Completitud**: Porcentaje de valores no nulos
- **Consistencia**: DetecciÃ³n de duplicados y formato
- **Validez**: DetecciÃ³n de outliers y tipos incorrectos
- **Integridad**: VerificaciÃ³n de referencias y restricciones

## ğŸ¯ Casos de Uso

### 1. E-commerce (MegaMercado)
```python
from config import ECOMMERCE_CONFIG, MEGAMERCADO_FILES

pipeline = DataCleaningPipeline(BASE_PATH)
clean_data = pipeline.run_complete_pipeline(MEGAMERCADO_FILES, ECOMMERCE_CONFIG)
```

### 2. Datos Financieros
```python
from config import FINANCIAL_CONFIG

config = FINANCIAL_CONFIG
# Personalizar segÃºn necesidades especÃ­ficas
```

### 3. Limpieza Estricta
```python
from config import apply_strategy_to_all_datasets, STRICT_CLEANING

datasets = ['ventas', 'clientes', 'productos']
config = apply_strategy_to_all_datasets(datasets, STRICT_CLEANING)
```

## ğŸš¨ Manejo de Errores

La pipeline incluye manejo robusto de errores:

- **Archivos faltantes**: ContinÃºa con otros datasets
- **Formato incorrecto**: Registra error y salta el archivo
- **Memoria insuficiente**: OptimizaciÃ³n automÃ¡tica de tipos
- **CodificaciÃ³n**: DetecciÃ³n automÃ¡tica de encoding

## ğŸ“ˆ OptimizaciÃ³n de Performance

### Para Datasets Grandes

```python
# ConfiguraciÃ³n optimizada para datasets grandes
config_optimizada = {
    'mi_dataset': {
        'missing_strategy': 'drop_columns',  # MÃ¡s rÃ¡pido
        'missing_threshold': 0.8,
        'remove_outliers': False,  # Desactivar si es muy lento
        'type_mapping': {
            'fecha': 'datetime64[ns]'  # MÃ¡s eficiente
        }
    }
}
```

### Monitoreo de Memoria

```python
# La pipeline automÃ¡ticamente reporta uso de memoria
pipeline = DataCleaningPipeline(BASE_PATH, log_level='DEBUG')
```

## ğŸ” Debugging

### Logs Detallados

```python
# Activar logging detallado
pipeline = DataCleaningPipeline(BASE_PATH, log_level='DEBUG')
```

### RevisiÃ³n Manual

```python
# Verificar datos antes de guardar
clean_data = pipeline.run_complete_pipeline(files, config)

for name, df in clean_data.items():
    print(f"{name}: {df.shape}")
    print(df.head())
    print(df.info())
```

## ğŸ“ Ejemplos Completos

Ejecutar los ejemplos incluidos:

```bash
# Ejecutar todos los ejemplos de limpieza
python ejemplo_uso_pipeline.py

# Ejecutar ejemplos de detecciÃ³n de inconsistencias
python ejemplo_deteccion_inconsistencias.py

# Solo ejemplo especÃ­fico (modificar el main())
python -c "from ejemplo_uso_pipeline import ejemplo_pipeline_completo; ejemplo_pipeline_completo()"
```

## ğŸ¤ ContribuciÃ³n

Para extender la pipeline:

1. **Nuevas estrategias**: AÃ±adir mÃ©todos a `DataCleaningPipeline`
2. **Configuraciones**: Agregar a `config.py`
3. **Validadores**: Implementar en `analyze_data_quality`
4. **Tipos de datos**: Extender `standardize_data_types`

## ğŸ“‹ Checklist de Limpieza

- [ ] âœ… Valores faltantes manejados
- [ ] âœ… Duplicados eliminados
- [ ] âœ… Outliers detectados y tratados
- [ ] âœ… Tipos de datos correctos
- [ ] âœ… Texto estandarizado
- [ ] ğŸ” **Inconsistencias detectadas y reportadas**
- [ ] ğŸš¨ **Reglas de negocio validadas**
- [ ] ğŸ”— **Integridad referencial verificada**
- [ ] âœ… Calidad validada
- [ ] âœ… Reporte generado
- [ ] âœ… Datos guardados

## ğŸ“ Archivos Generados

DespuÃ©s de ejecutar la pipeline con detecciÃ³n de inconsistencias:

```
datos_limpios/
â”œâ”€â”€ clientes_clean.csv           # Datos limpios
â”œâ”€â”€ productos_clean.csv
â”œâ”€â”€ ventas_clean.csv
â”œâ”€â”€ logistica_clean.csv
â”œâ”€â”€ proveedores_clean.csv
â”œâ”€â”€ cleaning_report.txt          # Reporte de limpieza
â”œâ”€â”€ inconsistencies_report.txt   # ğŸ†• Reporte detallado de inconsistencias
â””â”€â”€ data_cleaning.log           # Log completo del proceso
```

## ğŸ¯ Ejemplos de Inconsistencias Detectadas

### ğŸ“§ Formato de Email InvÃ¡lido
```
ğŸš¨ FORMAT_INCONSISTENCY
   Tabla: clientes
   Columna: email  
   DescripciÃ³n: Valores que no siguen el formato esperado de email
   Casos: 3
   Ejemplos: ['pedro@invalid', 'jose@', 'ana@']
   AcciÃ³n sugerida: Estandarizar formato o validar valores en columna email
```

### ğŸ”¢ Edad Fuera de Rango
```
ğŸš¨ RANGE_INCONSISTENCY
   Tabla: clientes
   Columna: edad
   DescripciÃ³n: Valores fuera del rango esperado (0-120)
   Casos: 2  
   Ejemplos: [-5, 200]
   AcciÃ³n sugerida: Verificar y corregir valores fuera de rango en edad
```

### ğŸ”— Referencia HuÃ©rfana
```
ğŸš¨ REFERENTIAL_INTEGRITY_VIOLATION
   Tabla: ventas
   Columna: id_cliente
   DescripciÃ³n: Referencias a clientes.id_cliente que no existen
   Casos: 2
   Ejemplos: [99, 88]
   AcciÃ³n sugerida: Eliminar o corregir referencias huÃ©rfanas en ventas.id_cliente
```

### â° Orden CronolÃ³gico Incorrecto
```
ğŸš¨ CHRONOLOGICAL_INCONSISTENCY  
   Tabla: ventas
   Columna: fecha_pedido vs fecha_entrega
   DescripciÃ³n: fecha_pedido deberÃ­a ser anterior a fecha_entrega
   Casos: 2
   Ejemplos: [{'fecha_pedido': '2024-01-15', 'fecha_entrega': '2024-01-10'}]
   AcciÃ³n sugerida: Verificar orden cronolÃ³gico entre fecha_pedido y fecha_entrega
```

## ğŸ‰ Â¡Listo!

Tu pipeline de limpieza estÃ¡ configurada y lista para usar. Los datos limpios estarÃ¡n en `datos_limpios/` junto con un reporte detallado del proceso.

---
*Desarrollado para el proyecto 365_AI_Journey - Caso MegaMercado*