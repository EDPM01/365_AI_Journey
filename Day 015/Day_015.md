Read this pappers about propose the Transformer: Attention Is All You Need


Read this pappers about propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.

<img width="566" alt="Captura de pantalla 2025-02-03 a la(s) 4 54 51â€¯p m" src="https://github.com/user-attachments/assets/5ee64b40-a9c6-49d9-876b-15ce48669c90" />
